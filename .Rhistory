, 'McNabb Recreation Centre', 'Central', '45.409026', '-75.702634'
, 'Minto Recreation Complex - Barrhaven', 'South', '45.253397', '-75.736022'
, 'Navan Memorial Centre', 'East', '45.421381', '-75.421326'
, 'Nepean Sportsplex', 'West End', '45.326912', '-75.746002'
, 'R.J. Kennedy Arena', 'East', '45.514531', '-75.402976'
, 'Ray Friel Recreation Complex', 'East', '45.471382', '-75.49173'
, 'Richmond Arena', 'South', '45.195657', '-75.837795'
, 'Sandy Hill Arena', 'Central', '45.419286', '-75.673804'
, 'St-Laurent Complex', 'East', '45.43638', '-75.647014'
, 'Stuart Holmes Arena', 'South', '45.147472', '-75.601273'
, 'Tom Brown Arena', 'Central', '45.408105', '-75.722154'
, 'W. Erskine Johnston Arena', 'West', '45.349145', '-76.038851'
, 'Walter Baker Sports Centre', 'West', '45.280326', '-75.761861'), ncol=4, byrow = T)
arenas <- rbind(arenas,
matrix(c('Bob MacQuarrie Recreation Complex - Orléans', 'East', '45.466459', '-75.545228'
, 'Brewer Pool', 'Central', '45.389361', '-75.691916'
, 'Canterbury Recreation Complex', 'South', '45.390558', '-75.628903'
, 'Champagne Pool & Fitness Centre', 'Central', '45.430659', '-75.686719'
, 'Dovercourt', 'West End', '45.383344', '-75.752209'
, 'Deborah Anne Kirwan Pool', 'South', '45.367799', '-75.656587'
, 'François Dupuis Recreation Centre', 'East', '45.457012', '-75.449182'
, 'Goulbourn Recreation Complex', 'West', '45.263237', '-75.907526'
, 'Jack Purcell Community Centre', 'Central', '45.415769', '-75.689533'
, 'Kanata Wave Pool', 'West', '45.311241', '-75.898782'
, 'Lowertown Pool', 'Central', '45.434277', '-75.681504'
, 'Minto Recreation Complex - Barrhaven', 'South', '45.253397', '-75.736022'
, 'Nepean Sportsplex', 'West End', '45.326912', '-75.746002'
, 'Pinecrest Recreation Complex', 'West End', '45.348021', '-75.773401'
, 'Plant Recreation Centre', 'Central', '45.40799', '-75.714452'
, 'Ray Friel Recreation Complex', 'East', '45.471382', '-75.49173'
, 'Richcraft Recreation Complex - Kanata', 'West', '45.344475', '-75.93036'
, 'Sawmill Creek Pool & Community Centre', 'South', '45.350423', '-75.636775'
, 'Splash Wave Pool', 'East', '45.437087', '-75.600821'
, 'St-Laurent Pool', 'East', '45.43638', '-75.647014'
, 'Walter Baker Sports Centre', 'West', '45.280326', '-75.761861'
, 'Y-Carlingwood', 'West End', '45.37274', '-75.769143'
, 'Y-Ruddy Family', 'East', '45.481451', '-75.508211'
, 'Y-Taggart Family', 'Central', '45.411486', '-75.689558'
, 'Y-Clarence Rockland', 'East', '45.550633', '-75.286126'), ncol=4, byrow = T))
colnames(arenas) <- c('Arena', 'Locale', 'Latitude', 'Longitude')
arenas <- as.data.frame(arenas, stringsAsFactors = F)
preprocess_data(type='skate', test=F)
library(XML)
library(RCurl)
library(stringr)
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
retrieve_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
preprocess_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
source("general.R")
# install.packages('RCurl')
# install.packages('XML')
# install.packages('stringr')
# install.packages('data.table')
# install.packages('lubridate')
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
preprocess_data(type='skate', test=F)
retrieve_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
preprocess_data(type='swim', test=F)
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
retrieve_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
preprocess_data(type='swim', test=F)
retrieve_data(type='skate', test=F)
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
retrieve_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
preprocess_data(type='swim', test=F)
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(get_url)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-03-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
retrieve_data(type='skate', test=F)
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-03-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-12-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-08-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-09-30'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
x
preprocess_data(type='skate', test=F)
x
traceback
parse_cancellations <- function(x) {
if (!grepl(x = x["Comments"], pattern = 'Last session')) {
all_cancels <- data.frame()
for(m in months) {
can <- str_extract_all(x["Comments"], paste(m, " [0-9, ]+", sep=""))
if(!is.na(can)) {
cancel <- str_extract_all(can, "[0-9]+")
canceldates <- as.Date(paste('2016-', m, '-', cancel[[1]], sep=''), format = "%Y-%b-%d")
canceldates <- as.Date(ifelse (as.Date(canceldates) < Sys.Date(), canceldates + years(1), canceldates), origin = '1970-1-1')
#     Join everything together into a single data.frame
for(d in canceldates) {
all_cancels <- rbind(all_cancels,
data.frame(x["ID"], x["StartDate"], as.Date(d, origin="1970-01-01"), stringsAsFactors = FALSE))
}
}
}
if (nrow(all_cancels) > 0) {
colnames(all_cancels) <- c('ID', 'Start', 'CancelDate')
all_cancels$CancelDate <- as.Date(ifelse(all_cancels$CancelDate < all_cancels$Start, all_cancels$CancelDate + years(1), all_cancels$CancelDate), origin = '1970/1/1')
all_cancels
}
}
}
preprocess_data(type='skate', test=F)
x
charToDate(x)
charToDate('10-31-2016')
charToDate('2016-10-31')
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-08-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
preprocess_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='swim', test=F)
