page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='swim', test=F)
preprocess_data(type='skate', test=F)
x
preprocess_data(type='skate', test=F)
x
traceback
parse_cancellations <- function(x) {
if (!grepl(x = x["Comments"], pattern = 'Last session')) {
all_cancels <- data.frame()
for(m in months) {
can <- str_extract_all(x["Comments"], paste(m, " [0-9, ]+", sep=""))
if(!is.na(can)) {
cancel <- str_extract_all(can, "[0-9]+")
canceldates <- as.Date(paste('2016-', m, '-', cancel[[1]], sep=''), format = "%Y-%b-%d")
canceldates <- as.Date(ifelse (as.Date(canceldates) < Sys.Date(), canceldates + years(1), canceldates), origin = '1970-1-1')
#     Join everything together into a single data.frame
for(d in canceldates) {
all_cancels <- rbind(all_cancels,
data.frame(x["ID"], x["StartDate"], as.Date(d, origin="1970-01-01"), stringsAsFactors = FALSE))
}
}
}
if (nrow(all_cancels) > 0) {
colnames(all_cancels) <- c('ID', 'Start', 'CancelDate')
all_cancels$CancelDate <- as.Date(ifelse(all_cancels$CancelDate < all_cancels$Start, all_cancels$CancelDate + years(1), all_cancels$CancelDate), origin = '1970/1/1')
all_cancels
}
}
}
preprocess_data(type='skate', test=F)
x
charToDate(x)
charToDate('10-31-2016')
charToDate('2016-10-31')
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-08-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
retrieve_data(type='skate', test=F)
preprocess_data(type='skate', test=F)
retrieve_data(type='swim', test=F)
preprocess_data(type='swim', test=F)
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
source("general.R")
source("general.R")
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
library(data.table)
#------------------------------------
#     Web Scraping Functions
#------------------------------------
get_url <- function(type='skate', page=0) {
paste0("http://ottawa.ca/2/en/drop-in-programs?activity-id=",
ifelse(type=='swim', 392, 391),
"&field_dropin_facility_target_id_selective=",
ifelse(type=='swim',"A","a"),
"ll&field_dropin_activity_type_target_id_selective=All",
ifelse(page==0, "", paste0("&&page=", page)))
}
retrieve_data <- function(type='skate', test=TRUE, page=0) {
all_results <- data.frame()
all_results_by_day <- data.frame()
#     Repeat until a single record is retrieved
repeat{
fileUrl <- get_url(type, page)
print(fileUrl)
doc <- htmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
child <- xmlChildren(rootNode)[[3]]
#     Retrieve each of the rows
#     First row only consists of column names so ignore it
rows <- getNodeSet(child, "//tr")[-1]
if (length(rows) <= 1 & is.na(xmlValue(rows[[1]][[3]]))) break
result_page <- do.call(rbind, lapply(rows, function(x) {
#     Parse out field values for each row and return data in same format as previously
id <- NA
location <- ifelse(is.null(xmlValue(x[[1]])), NA,
gsub(pattern = '\r\n', replacement = '', x = trim(str_split(string = xmlValue(x[[1]], trim=T), pattern='\n   Map')[[1]][1])))
#gsub(x = xmlValue(x[[1]], trim=T), pattern = ' \\nMap', replacement = ''))
day <- ifelse(is.null(xmlValue(x[[7]])), NA, xmlValue(x[[7]], trim=T))
start_date <- ifelse(xmlValue(x[[9]], trim=T)=='Ongoing', format(Sys.Date(), '%B %d'), xmlValue(x[[9]], trim=T))
#     Fudge end date since the site doesn't provide end date per session
end_date <- '2016-08-31'
starttime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][1]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][1])
endtime <- ifelse(is.null(str_split(xmlValue(x[[5]]), 'to')[[1]][2]), NA, str_split(xmlValue(x[[5]], trim=T), 'to')[[1]][2])
session_type <- ifelse(is.null(xmlValue(x[[3]])), NA,
gsub(pattern = 'Fee$', replacement = '', xmlValue(x[[3]], trim=T)))
comments <- gsub(pattern = '\\n', replacement = '', x = ifelse(is.null(xmlValue(x[[11]])), NA, xmlValue(x[[11]])))
#     Join everything together into a single data.frame
data.frame(id, location, day, start_date, end_date, starttime, endtime, session_type, comments, stringsAsFactors = FALSE)
}))
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(test || page > 30) break
}
all_results$id <- seq(1, nrow(all_results))
#     The city feed started to combine multiple weekday schedules into a single record
#     Cycle through each record, matching on the Day of the Week string, and insert into
#     separate aggregate table
#     This will handle normal records with one day and records with multiple days
for (d in daynames) {
temp <- all_results[grepl(d, all_results$day), ]
if (nrow(temp) > 0) {
temp$day <- d
all_results_by_day <- rbind(all_results_by_day, temp)
}
}
colnames(all_results_by_day) <- c('ID', 'Arena', 'Day', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'SessionType', 'Comments')
#     Clean up Sawmill Arena entries
if (type=='swim')
{
all_results_by_day[grepl(pattern = 'Sawmill Creek Pool', x = all_results_by_day$Arena),]$Arena <- c('Sawmill Creek Pool & Community Centre')
}
if(test) {
all_results_by_day
} else {
write.csv(x = all_results_by_day, file = paste0(folder_raw, 'data_', type, '.csv'), fileEncoding = "latin1")
}
}
#     retrieve_data(type='skate', test=F)
#     retrieve_data(type='swim', test=F)
retrieve_data(type='skate', test=T)
retrieve_data()
source("general.R")
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
activity_group <- as.data.frame(
matrix(c('Skating', '593',
'Swimming', '589',
'Aquafit', '608',
'Spinning', '617',
'Fitness', '616',
'Yoga', '607',
'Sports', '678'), ncol = 2, byrow = T))
colnames(activity_group) <- c('name' ,'code')
#------------------------------------
#     Web Scraping Functions
#------------------------------------
get_city_url <- function(p_activity = 'Skating',
p_page=0) {
activities_url <- paste0('http://ottawa.ca/2/en/residents/parks-and-recreation/drop-in-programs?f[0]=field_diss_activity%3A', activity_group[activity_group==p_activity,c('code')])
if(p_page>0) {
activities_url <- paste0(activities_url, '&page=', as.character(p_page))
}
activities_url
}
#     get_city_url(p_page=2)
retrieve_data <- function(p_activity = 'Skating', p_test = TRUE) {
all_results <- data.frame()
page = 0
#     Repeat until a single record is retrieved
repeat {
city_url <- get_city_url(p_activity, page)
print(city_url)
#   For some reason, need to use httr::GET
#   htmlTreeParse retrieved the wrong data as if query string was ignored)
acts <- httr::GET(city_url)
doc <- htmlTreeParse(file = acts, useInternal = TRUE, isURL = TRUE, encoding = 'UTF-8')
activities <- readHTMLTable(doc, as.data.frame = TRUE, trim = TRUE)
#   Grab node_url to enable linking to the city's actual Facility/Activity page
if(length(activities) > 0) {
result_page <- cbind(activities[[1]], node_url=xpathSApply(doc, '//td[@class="views-field views-field-field-diss-repeating"]//a/@href'))
result_page[,1] <- trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue))
} else {
break
}
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(p_test & page > 4) break
}
names(all_results) <- c('Facility', 'Activity', 'Date', 'Time', 'Details', 'node_url')
if(p_test) {
all_results
} else {
saveRDS(object = all_results, file = paste0(folder_raw, p_activity, '_', gsub('-', '', ymd(Sys.Date())), '.rds'))
}
}
#   retrieve_data(p_activity='Skating', p_test=FALSE)
#   retrieve_data(p_activity='Swimming', p_test=FALSE)
retrieve_data()
retrieve_data(p_activity='Skating', p_test=FALSE)
retrieve_data(p_activity='Swimming', p_test=FALSE)
debug(retrieve_data)
retrieve_data(p_activity='Swimming', p_test=FALSE)
activities
activities[[1]]
xpathSApply(doc, '//td[@class="views-field views-field-field-diss-repeating"]//a/@href')
xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue)
activities
str(activities0)
str(activities)
str(activities)
activities
activities[date=='']
activities[date=='',]
activities[Date=='',]
str(activities)
str(activities[[1]])
str(activities[[1]][Date==''])
str(activities[[1]][Date=='',])
str(activities[[1]][Date=='',])
str(activities[[1]])
result_page
str(result_page)
result_page$`
Date          `
result_page$`
Date          `
str(result_page$`
Facility          `)
trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue)
trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue)
trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue)
trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue)
trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue))
result_page
result_page[Date=='',]
result_page[result_page$Date=='',]
result_page[is.na(result_page$Date),]
result_page[is.null(result_page$Date),]
result_page$Date
result_page
result_page$`
Date          `
result_page$[result_page$`
Date          `=='',]
result_page$[result_page$`
Date          `=='',]
result_page[result_page$`
Date          `=='',]
activities[Activities$`
Date          `=='',]
activities[activities$`
Date          `=='',]
activities[[1]][activities[[1]]$`
Date          `=='',]
result_page
result_page[result_page$node_url!='/2/en/node/null'.]
result_page[result_page$node_url != '/2/en/node/null',]
retrieve_data <- function(p_activity = 'Skating', p_test = TRUE) {
all_results <- data.frame()
page = 0
#     Repeat until a single record is retrieved
repeat {
city_url <- get_city_url(p_activity, page)
print(city_url)
#   For some reason, need to use httr::GET
#   htmlTreeParse retrieved the wrong data as if query string was ignored)
acts <- httr::GET(city_url)
doc <- htmlTreeParse(file = acts, useInternal = TRUE, isURL = TRUE, encoding = 'UTF-8')
activities <- readHTMLTable(doc, as.data.frame = TRUE, trim = TRUE)
#   Grab node_url to enable linking to the city's actual Facility/Activity page
if(length(activities) > 0) {
result_page <- cbind(activities[[1]], node_url=xpathSApply(doc, '//td[@class="views-field views-field-field-diss-repeating"]//a/@href'))
#   Exclude any empty row. This is caused by a "one time event" that has no other column data
#   and causes an error when trying to replace the arena name
result_page <- result_page[result_page$node_url != '/2/en/node/null',]
result_page[,1] <- trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue))
} else {
break
}
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(p_test & page > 4) break
}
names(all_results) <- c('Facility', 'Activity', 'Date', 'Time', 'Details', 'node_url')
if(p_test) {
all_results
} else {
saveRDS(object = all_results, file = paste0(folder_raw, p_activity, '_', gsub('-', '', ymd(Sys.Date())), '.rds'))
}
}
debug(retrieve_data)
retrieve_data(p_activity='Swimming', p_test=FALSE)
result_page
retrieve_data <- function(p_activity = 'Skating', p_test = TRUE) {
all_results <- data.frame()
page = 0
#     Repeat until a single record is retrieved
repeat {
city_url <- get_city_url(p_activity, page)
print(city_url)
#   For some reason, need to use httr::GET
#   htmlTreeParse retrieved the wrong data as if query string was ignored)
acts <- httr::GET(city_url)
doc <- htmlTreeParse(file = acts, useInternal = TRUE, isURL = TRUE, encoding = 'UTF-8')
activities <- readHTMLTable(doc, as.data.frame = TRUE, trim = TRUE)
#   Grab node_url to enable linking to the city's actual Facility/Activity page
if(length(activities) > 0) {
result_page <- cbind(activities[[1]], node_url=xpathSApply(doc, '//td[@class="views-field views-field-field-diss-repeating"]//a/@href'))
#   Exclude any empty row. This is caused by a "one time event" that has no other column data
#   and causes an error when trying to replace the arena name
result_page <- result_page[result_page$node_url != '/2/en/node/null',]
result_page[,1] <- trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue))
} else {
break
}
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(p_test & page > 4) break
}
names(all_results) <- c('Facility', 'Activity', 'Date', 'Time', 'Details', 'node_url')
if(p_test) {
all_results
} else {
saveRDS(object = all_results, file = paste0(folder_raw, p_activity, '_', gsub('-', '', ymd(Sys.Date())), '.rds'))
}
}
retrieve_data(p_activity='Swimming', p_test=FALSE)
write_tableau_data('Skating')
source("general.R")
library(lubridate)
library(stringr)
library(RCurl)
library(XML)
activity_group <- as.data.frame(
matrix(c('Skating', '593',
'Swimming', '589',
'Aquafit', '608',
'Spinning', '617',
'Fitness', '616',
'Yoga', '607',
'Sports', '678'), ncol = 2, byrow = T))
colnames(activity_group) <- c('name' ,'code')
#------------------------------------
#     Web Scraping Functions
#------------------------------------
get_city_url <- function(p_activity = 'Skating',
p_page=0) {
activities_url <- paste0('http://ottawa.ca/2/en/residents/parks-and-recreation/drop-in-programs?f[0]=field_diss_activity%3A', activity_group[activity_group==p_activity,c('code')])
if(p_page>0) {
activities_url <- paste0(activities_url, '&page=', as.character(p_page))
}
activities_url
}
#     get_city_url(p_page=2)
retrieve_data <- function(p_activity = 'Skating', p_test = TRUE) {
all_results <- data.frame()
page = 0
#     Repeat until a single record is retrieved
repeat {
city_url <- get_city_url(p_activity, page)
print(city_url)
#   For some reason, need to use httr::GET
#   htmlTreeParse retrieved the wrong data as if query string was ignored)
acts <- httr::GET(city_url)
doc <- htmlTreeParse(file = acts, useInternal = TRUE, isURL = TRUE, encoding = 'UTF-8')
activities <- readHTMLTable(doc, as.data.frame = TRUE, trim = TRUE)
#   Grab node_url to enable linking to the city's actual Facility/Activity page
if(length(activities) > 0) {
result_page <- cbind(activities[[1]], node_url=xpathSApply(doc, '//td[@class="views-field views-field-field-diss-repeating"]//a/@href'))
#   Exclude any empty row. This is caused by a "one time event" that has no other column data
#   and causes an error when trying to replace the arena name
result_page <- result_page[result_page$node_url != '/2/en/node/null',]
result_page[,1] <- trim(xpathSApply(doc, '//td[@class="views-field views-field-field-diss-facility"]//a/text()', xmlValue))
} else {
break
}
#     Copy page of results into master data frame
all_results <- rbind(all_results, result_page)
#     Move to the next set of results
page <- page + 1
if(p_test & page > 4) break
}
names(all_results) <- c('Facility', 'Activity', 'Date', 'Time', 'Details', 'node_url')
if(p_test) {
all_results
} else {
saveRDS(object = all_results, file = paste0(folder_raw, p_activity, '_', gsub('-', '', ymd(Sys.Date())), '.rds'))
}
}
#   retrieve_data(p_activity='Skating', p_test=FALSE)
#   retrieve_data(p_activity='Swimming', p_test=FALSE)
#---------------------------
#     Tidy Data
#---------------------------
load_data_raw <- function(p_activity='Skating') {
file <- max(list.files(path = folder_raw, pattern = p_activity, full.names = TRUE))
as.data.table(readRDS(file))
}
format_data <- function(p_activity='Skating') {
data <- load_data_raw(p_activity)
#----------------------------
#     Format dates
#----------------------------
#     Append current year to Date...
data$Date <- as.Date(paste0(data$Date, ', ', format(Sys.Date(), '%Y')), format = '%A, %B %d, %Y')
#     ...then add 1 year for any dates in the past
data[data$Date < Sys.Date(),]$Date <- data[data$Date < Sys.Date(), ]$Date + years(1)
data <- cbind(data, trim(str_split(data$Time, '-', simplify = TRUE)))
setnames(data, old = c('V1', 'V2'), new=c('StartTime', 'EndTime'))
#----------------------------------------------------------------------------------
#     Join to Sessions lookup to shorten SessionType and retrieve SessionGroup
#----------------------------------------------------------------------------------
data$Activity <- gsub(pattern = paste0(p_activity, ' - '), replacement = '', x = data$Activity)
data <- merge(data, sessions, by = "Activity", all.x = TRUE)
data$SessionType <- data$ShortName
#----------------------------------------------------
#     Join to Arenas lookup to retrieve Locale
#----------------------------------------------------
data <- merge(data, facilities, by = "Facility", all.x = TRUE)
data[, .(Date, StartTime, EndTime, ActivityID, FacilityID)]
}
#  format_data()
write_tableau_data <- function(p_activity='Skating') {
write.table(x = format_data(p_activity), file = paste0(folder_clean, p_activity, '.csv'), sep = ',', row.names = FALSE)
}
#   write_tableau_data('Skating')
#   write_tableau_data('Swimming')
write_tableau_data('Skating')
write_tableau_data('Swimming')
